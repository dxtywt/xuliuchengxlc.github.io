---

layout: post
title: 理解梯度下降
tags: mathematics
author: Liucheng Xu
disqus: "y"
published: true

---

* TOC
{:toc}

机器学习中常会用随机梯度下降法求解一个目标函数的优化问题，我们所追求的是目标函数能够快速收敛或到达一个极小值点。而随机梯度法操作起来也很简单，不过是求偏导数而已，但是为什么是这样呢？为什么算个偏导数就能说下降得最快？初期并不很明了，后来看过一些数学相关的知识才稍微明白了一点，一下内容算是一个理解梯度的渐进过程。如果不当之处，欢迎指正。

以下关于导数，偏导数的内容可在维基百科中找到，关于方向导数与梯度的内容可在高等数学书中找到。如有遇到公式无法正常显示，请在将鼠标停留在公式处右击**Math Settings**中选择**Math Renderer**,选择**SVG**即可。

## 导数

导数（Derivative）是微积分学中重要的基础概念。一个函数在某一点的导数描述了这个函数在这一点附近的**变化率**。导数的本质是通过**极限**的概念对函数进行局部的线性逼近。



当函数 $f$ 的自变量在一点 $x_0$ 上产生一个增量 $h$ 时， 
函数输出值的增量与自变量增量$h$的比值在$h$趋于$0$时的极限如果存在，即为$f$在$x_0$处的导数，记作$f'(x_0)$、$\frac{\mathrm{d}f}{\mathrm{d}x}(x_0)$或$\left.\frac{\mathrm{d}f}{\mathrm{d}x}\right|_{x=x_0}$.

**几何意义上导数表示函数在这一点切线的斜率。**

## 偏导数

在数学中，一个**多变量**的函数的偏导数是它关于其中一个变量的导数，而**保持其他变量恒定**（相对于全导数，在其中所有变量都允许变化）。

假设$ƒ$是一个多元函数。例如：

 $$z = f(x, y) = x^2 + xy + y^2$$

$f = x^2 + xy + y^2$的图像。我们希望求出函数在点（1, 1, 3）的对x的偏导数；对应的切线与xOz平面平行。
因为曲面上的每一点都有无穷多条切线，描述这种函数的导数相当困难。**偏导数就是选择其中一条切线**，并求出它的斜率。通常，最感兴趣的是垂直于y轴（平行于xOz平面）的切线，以及垂直于x轴（平行于yOz平面）的切线。

![partial](/images/blog/2016/04-15/partial.png)

一种求出这些切线的好办法是把其他变量视为常数。例如，欲求出以上的函数在点（1, 1, 3）的与xOz平面平行的切线。上图中显示了函数$f = x^2 + xy + y^2$的图像以及这个平面。下图中显示了函数在平面y = 1上是什么样的。我们把变量y视为常数，通过对方程求导，我们发现$ƒ$在点（x, y, z）的。我们把它记为：
$\frac{\partial z}{\partial x} = 2x+y$，于是在点（1, 1, 3）的与xOz平面平行的切线的斜率是3。$\frac{\partial f}{\partial x} = 3$ 在点（1, 1, 3），或称“f在（1, 1, 3）的关于x的偏导数是3”。

![partial](/images/blog/2016/04-15/y=1.png)

**在几何意义上偏导数即为函数在坐标轴方向上的变化率。**

## 方向导数

方向导数是分析学特别是多元微积分中的概念。一个标量场在某点沿着某个向量方向上的方向导数，描绘了该点附近标量场沿着该向量方向变动时的瞬时变化率。方向导数是偏导数的概念的推广。

![partial](/images/blog/2016/04-15/zh.png)

**几何意义上方向导数为函数在某点沿着其他特定方向上的变化率**。

## 梯度

在一个数量场中，函数在给定点处沿不同的方向，其方向导数一般是不相同的。那么沿着哪一个方向其方向导数最大，其最大值为多少，为此引进一个很重要的概念--梯度。**函数在点$p_0$处沿哪一方向增加的速度最快？**

![partial](/images/blog/2016/04-15/gd.png)

## 方向导数与梯度的关系

![partial](/images/blog/2016/04-15/gd2.png)

![partial](/images/blog/2016/04-15/gd3.png)

## 总结

函数在某一点处的方向导数在其梯度方向上达到最大值，此最大值即梯度的范数。

这就是说，**沿梯度方向，函数值增加最快**。同样可知，方向导数的最小值在梯度的相反方向取得，此最小值为最大值的相反数，从而**沿梯度相反方向函数值的减少最快**。详细内容：[方向导数与梯度](http://math.fudan.edu.cn/gdsx/KEJIAN/%E6%96%B9%E5%90%91%E5%AF%BC%E6%95%B0%E5%92%8C%E6%A2%AF%E5%BA%A6.pdf)。

在机器学习中往往是最小化一个目标函数L，理解了上面的内容，便很容易理解在SGD中常用的更新公式：

$$\theta = \theta - \frac{\partial L}{\partial \theta}$$

在更新完参数$\theta$以后也就是算出了目标函数的梯度，在此过程中也便是达到了函数值减少最快的效果，经过迭代以后目标函数即可很快地到达一个极小值。
